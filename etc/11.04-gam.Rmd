```{r, message=FALSE}
library(showtext)
showtext_auto(enable = TRUE)
font_add("自訂字體", "/Users/mac/Documents/in_sync_mac/in_sync_documents/fonts/TaipeiSansTCBeta-Regular.ttf")
```

```{r, message=FALSE}
library(tidyverse)
library(mgcv)
library(psych)

set.seed(1)
```

### Data preparation
```{r, message=FALSE}
library(arrow)

fp <- "/Users/mac/Documents/GitHub/GIL/CCKF/dietcoke/data/chunked_freq/20211110-111217/gam_df.parquet"
if(file.exists(fp)){df <- read_parquet(fp)}

df$text_slice <- as.numeric(df$text_slice)
```

- To check whether there are missing values in the `mid_year` column
```{r}
any(is.na(df$mid_year)) == FALSE
all(!is.na(df$mid_year)) == TRUE
"wb114254" %in% df$urn == FALSE
```

- How many texts (identified by `urn`) are included?
```{r}
length(unique(df$urn))
```

- Filtering out some more characters
```{r}
#chars <- sample(c("子","曰","者","於","為","有","其","人","一","而","以","也","不","之"), 5)
#chars <- sample(unique(df$char), 5)
#chars <- c(sample(unique(df$char), 5), chars)
chars <- unique(df$char)

df <- df %>% filter(char %in% chars)
char_df <- data.frame(char=chars, char_id=1:length(chars))
df <- left_join(df, char_df)
```

### Cleaning
```{r}
kdf <- df %>%
  select(mid_year, urn, text_slice) %>%
  distinct() %>%
  arrange(mid_year, urn, text_slice) %>%
  mutate(k=row_number())

dynaspan_levels <- c("先秦", "漢", "魏晉南北", "唐五代十國", "宋元", "明", "清", "民國")
dynaspan_yearto_levels <- c(-221, 220, 589, 1125, 1368, 1644, 1911, 1949)
dynaspan_df <- data.frame(
  dynaspan=dynaspan_levels,
  dynaspan_yearto=dynaspan_yearto_levels
)

df <- left_join(df, kdf) %>%
  left_join(dynaspan_df)

df$dynaspan <- factor(df$dynaspan, levels=dynaspan_levels)
```

```{r}
var(df$raw_freq)
mean(df$raw_freq)

df %>% filter(raw_freq == 0) %>% nrow() / nrow(df)

df$logfreq <- log(df$raw_freq+1)

ggplot(df, aes(raw_freq)) + geom_histogram(binwidth=1)
ggplot(df, aes(logfreq)) + geom_histogram()

par(mfrow=c(2,1))
plot(density(df$raw_freq))
plot(density(df$logfreq))
```

```{r}
par(mfrow=c(2,1))
plot(density(df$mid_year))
abline(v=dynaspan_yearto_levels, col="lightblue")
rug(df$mid_year, ticksize=-0.1, col="blue") # side=3

df$mid_year_scaled <- scale(df$mid_year)
plot(density(df$mid_year_scaled))

df %>% select(dynaspan, k, urn) %>% distinct() %>%
  group_by(dynaspan) %>%
  summarize(`Number of texts`=length(unique(urn)), `Number of text slices`=length(k))
```
```
library(readr)
write_csv(df, "gam_df2.csv")
```

```{r}
df %>%
  select(char_id, dynaspan, mid_year, urn, text_slice, k, author_norm, raw_freq, logfreq) %>%
  lowerCor()
```

### clustering
```{r}
folder <- "/Users/mac/Documents/GitHub/GIL/CCKF/dietcoke/data/chunked_freq/20211110-111217/gam_models"
models <- lapply(list.files(folder, full=TRUE), readRDS)
```

```{r}
gam_df <- data.frame(
  formula=lapply(models, function(x){
    formula <- as.character(x$formula)
    formula <- paste(formula[[2]], formula[[1]], formula[[3]])
    return(formula)
  }) %>% unlist(),
  AIC=lapply(models, AIC) %>% unlist(),
  BIC=lapply(models, BIC) %>% unlist()
) %>%
  mutate(model_id=row_number()) %>%
  arrange(AIC, formula) %>%
  select(model_id, AIC, BIC, formula); gam_df

coef_mats <- lapply(models, function(x){
  coefs <- coef(x)
  coefs <- coefs[2:length(coefs)]
  coef_mat <- matrix(coefs, nrow=length(chars))
  return(coef_mat)
})

coef_dfs <- lapply(coef_mats, function(x){
  coef_df <- data.frame(x)
  rownames(coef_df) <- chars
  return(coef_df)
})

coef_mat <- coef_mats[[6]]
coef_df <- coef_dfs[[6]]
```
#### Distance (Dissimilarity)
```{r}
library(factoextra)
res.dist <- get_dist(coef_df, stand=FALSE, method = "euclidean")
# Compared to the standard dist() function, it supports correlation-based distance measures including “pearson”, “kendall” and “spearman” methods.

fviz_dist(res.dist, 
   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

#### K-means
```{r}
for(method in c("silhouette", "wss", "gap_stat")){
  print(fviz_nbclust(coef_mat, kmeans, method=method))
}
```

```{r}
km.res <- kmeans(coef_df, 2, nstart = 25)

fviz_cluster(km.res, data = coef_df,
             # ellipse.type = "norm",
             # Character specifying frame type. Possible values are 'convex', 'confidence' or types supported by stat_ellipse including one of c("t", "norm", "euclid").
             palette = "jco",
             ggtheme = theme_minimal())
```

#### PAM
```{r}
library(cluster)

pam.res <- pam(coef_df, 2)
fviz_cluster(pam.res,
             palette = "jco",
             ggtheme = theme_minimal())
```

#### hierarchical clustering
```{r}
res.hc <- hclust(dist(coef_df), method="ward.D2")

fviz_dend(res.hc, k = 10,
          cex = 0.5,
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, # color labels by groups
          rect = TRUE # add rectangle around groups
          )
```

```{r}
res.hc <- coef_df %>%
  # scale() %>%
  eclust("hclust", k = 2, graph = FALSE)

# Visualize with factoextra
fviz_dend(res.hc, 
          cex = 0.5,
          palette = "jco",
          rect = TRUE)

fviz_silhouette(res.hc)

# Silhouette width of observations
sil <- res.hc$silinfo$widths[, 1:3]

# Objects with negative silhouette
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]
```
#### fuzzy clustering
```{r}
res.fanny <- fanny(coef_df, k=2)

fviz_cluster(res.fanny,
             palette = "jco", ggtheme = theme_minimal())

fviz_silhouette(res.fanny, palette = "jco",
                ggtheme = theme_minimal())
```
#### model-based clustering
```{r}
library(ggpubr)

ggscatter(coef_df, x = "X1", y = "X3") +
  geom_density2d()

library(mclust)
mc <- Mclust(coef_df)
summary(mc)

mc$classification

# BIC values used for choosing the number of clusters
fviz_mclust(mc, "BIC", palette = "jco")
# Classification: plot showing the clustering
fviz_mclust(mc, "classification", geom = "point", 
            pointsize = 1.5, palette = "jco")
# Classification uncertainty
fviz_mclust(mc, "uncertainty", palette = "jco")
```

#### density-based clustering
```{r}
library(fpc)

db <- fpc::dbscan(coef_df, eps = 5, MinPts = 2)
fviz_cluster(db, data = coef_df, stand = FALSE,
             geom = "point", palette = "jco", ggtheme = theme_classic())

dbscan::kNNdistplot(coef_df, k =  5)
```

##### ordered dissimilarity image (ODI)
```{r}
gradient.color <- list(low = "steelblue",  high = "white")

coef_mat %>%
  # scale() %>%
  get_clust_tendency(n = 2, gradient = gradient.color)
```

```
concurvity(models[[6]])
```

Source: https://www.datanovia.com/en/blog/types-of-clustering-methods-overview-and-quick-start-r-code/